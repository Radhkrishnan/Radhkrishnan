###UK Biobank Modelling

import pandas as pd
import xgboost as xgb
import lightgbm as lgb
from sklearn.linear_model import Lasso
from sklearn.metrics import make_scorer, roc_auc_score, roc_curve, auc, confusion_matrix, recall_score, precision_score, f1_score, balanced_accuracy_score
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
import numpy as np
import time
import copy
import matplotlib.pyplot as plt

# Load the dataset
X = pd.read_csv("pp_98.csv")

# Extract the label column and encode 'Control' as 0 and 'CRC' as 1
y = X['casecontrol']

# Drop the label column from the feature set
X = X.drop('casecontrol', axis=1)

### TRAIN TEST SPLIT ###
# Split the data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, shuffle=True, stratify=y, random_state=50)

file_name = ["UKBB98"]

model_name = ["XGB", "LGBM", "LASSO"]

# Initialize the parameter grids for hyperparameter tuning
param_grids = {
    'xgb': {
        'n_estimators': list(range(100, 500, 100)),
        'max_depth': [5, 10, 20, 30],
        'learning_rate': [0.1, 0.01, 0.05],
        'subsample': [0.8, 1.0],
        'colsample_bytree': [0.8, 1.0]
    },
    'lgb': {
        'num_leaves': [31, 63, 127],
        'max_depth': [5, 10, 20, 30],
        'learning_rate': [0.1, 0.01, 0.05],
        'subsample': [0.8, 1.0],
        'colsample_bytree': [0.8, 1.0]
    },
    'lasso': {
        'alpha': list(np.logspace(-3, 3, 7))
    }
}

# Initialize the models
models = {
    'xgb': xgb.XGBClassifier(objective='binary:logistic', seed=50),
    'lgb': lgb.LGBMClassifier(objective='binary', seed=50),
    'lasso': Lasso(random_state=50, max_iter=10000)
}

# Initialize the data structure to store classifier results and metrics
classifier_data = {
    'xgb': {
        'grid_best_mscore': None,
        'grid_best_std': None,
        'classifier': None,
        'fpr': None,
        'tpr': None,
        'auc_score': None,
        'y_pred_prob': None,
        'ci': None
    },
    'lgb': {
        'grid_best_mscore': None,
        'grid_best_std': None,
        'classifier': None,
        'fpr': None,
        'tpr': None,
        'auc_score': None,
        'y_pred_prob': None,
        'ci': None
    },
    'lasso': {
        'grid_best_mscore': None,
        'grid_best_std': None,
        'classifier': None,
        'fpr': None,
        'tpr': None,
        'auc_score': None,
        'y_pred_prob': None,
        'ci': None
    }
}

dataset_classifier_data = {}

def calculate_ci(y_true, y_pred_prob, confidence_level=0.95):
    auc_scores = []
    n_bootstraps = 1000
    rng = np.random.RandomState(seed=42)
    for _ in range(n_bootstraps):
        indices = rng.randint(0, len(y_pred_prob), len(y_pred_prob))
        if len(np.unique(y_true[indices])) < 2:
            continue
        score = roc_auc_score(y_true[indices], y_pred_prob[indices])
        auc_scores.append(score)
    sorted_scores = np.sort(auc_scores)
    lower_bound = sorted_scores[int((1.0 - confidence_level) / 2.0 * len(sorted_scores))]
    upper_bound = sorted_scores[int((1.0 + confidence_level) / 2.0 * len(sorted_scores))]
    return lower_bound, upper_bound

def calculate_metric_ci(y_true, y_pred, metric_func, confidence_level=0.95, n_bootstraps=1000):
    metric_scores = []
    rng = np.random.RandomState(seed=42)
    for _ in range(n_bootstraps):
        indices = rng.randint(0, len(y_true), len(y_true))
        if len(np.unique(y_true[indices])) < 2:
            continue
        score = metric_func(y_true[indices], y_pred[indices])
        metric_scores.append(score)
    sorted_scores = np.sort(metric_scores)
    lower_bound = sorted_scores[int((1.0 - confidence_level) / 2.0 * len(sorted_scores))]
    upper_bound = sorted_scores[int((1.0 + confidence_level) / 2.0 * len(sorted_scores))]
    return lower_bound, upper_bound

print(file_name[0])
print("="*60)
print()

for i, classifier in enumerate(classifier_data):
    print(model_name[i])

    base_model = models[classifier]
    param_grid = param_grids[classifier]

    scorer = make_scorer(roc_auc_score, needs_threshold=True)
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=50)

    # Start recording the time for grid search to run
    time_start = time.time()

    # Run grid search
    grid_search = GridSearchCV(estimator=base_model, param_grid=param_grid,
                               scoring=scorer, refit=True, cv=skf, n_jobs=-1)

    grid_search.fit(X_train.values, y_train)

    # Stop recording
    time_grid = time.time() - time_start

    best_grid_model = grid_search.best_estimator_
    best_grid_model_params = grid_search.best_params_
    best_idx = np.argsort(grid_search.cv_results_['mean_test_score'])[-1]
    best_grid_model_score = grid_search.cv_results_['mean_test_score'][best_idx]
    best_grid_model_std = grid_search.cv_results_['std_test_score'][best_idx]

    print(f"Grid Search took {time_grid/60:.2f} mins ({time_grid:.2f} secs)")
    print(f"Best parameters: {best_grid_model_params}")
    print(f"Grid Validation AUC score: {best_grid_model_score}")
    print(f"Grid Validation std: {best_grid_model_std}")

    classifier_data[classifier]['classifier'] = best_grid_model
    classifier_data[classifier]['grid_best_mscore'] = best_grid_model_score
    classifier_data[classifier]['grid_best_std'] = best_grid_model_std

    # Predict and evaluate model performance (AUC)
    y_pred_prob = best_grid_model.predict(X_test.values) if i == 2 else best_grid_model.predict_proba(X_test.values)[:, 1]
    classifier_data[classifier]['y_pred_prob'] = y_pred_prob
    fpr_num, tpr_num, _ = roc_curve(y_test, y_pred_prob)
    classifier_data[classifier]['fpr'] = fpr_num
    classifier_data[classifier]['tpr'] = tpr_num
    auc_score_num = auc(fpr_num, tpr_num)
    classifier_data[classifier]['auc_score'] = auc_score_num

    # Calculate confidence intervals for AUC
    lower_ci, upper_ci = calculate_ci(y_test.values, y_pred_prob, confidence_level=0.95)
    classifier_data[classifier]['ci'] = (lower_ci, upper_ci)

    # Calculate other performance metrics
    true_labels = y_test.values
    predicted_labels = best_grid_model.predict(X_test.values)
    if i == 2:
        # Apply 0.5 threshold to classify as 1 or 0
        predicted_labels = np.where(y_pred_prob >= 0.5, 1, 0)

    # Calculate confusion matrix
    confusion = confusion_matrix(true_labels, predicted_labels)
    TP = confusion[1, 1]
    TN = confusion[0, 0]
    FP = confusion[0, 1]
    FN = confusion[1, 0]

    # Specificity
    specificity = TN / (TN + FP)
    # Recall
    recall = recall_score(true_labels, predicted_labels)
    # Precision
    precision = precision_score(true_labels, predicted_labels)
    # F1 Score
    f1 = f1_score(true_labels, predicted_labels)
    # Balanced Accuracy
    balanced_accuracy = balanced_accuracy_score(true_labels, predicted_labels)

    # Calculate confidence intervals
    recall_ci = calculate_metric_ci(true_labels, predicted_labels, recall_score)
    specificity_ci = calculate_metric_ci(true_labels, predicted_labels, lambda y_true, y_pred: TN / (TN + FP))
    precision_ci = calculate_metric_ci(true_labels, predicted_labels, precision_score)
    f1_ci = calculate_metric_ci(true_labels, predicted_labels, f1_score)

    print("Test AUC score: ", auc_score_num)
    print("AUC CI: ", classifier_data[classifier]['ci'])
    print("Specificity:", specificity)
    print("Specificity CI:", specificity_ci)
    print("Sensitivity (Recall):", recall)
    print("Recall CI:", recall_ci)
    print("Precision:", precision)
    print("Precision CI:", precision_ci)
    print("F1 Score:", f1)
    print("F1 Score CI:", f1_ci)
    print("Balanced Accuracy:", balanced_accuracy)
    print("-"*40)

dataset_classifier_data[file_name[0]] = copy.deepcopy(classifier_data)
print()

# Plot ROC Curves
plt.figure(figsize=(10, 8))
for model_key in classifier_data:
    plt.plot(classifier_data[model_key]['fpr'], classifier_data[model_key]['tpr'], label=f'{model_key.upper()} (AUC = {classifier_data[model_key]["auc_score"]:.2f} Â± {classifier_data[model_key]["grid_best_std"]:.2f}, CI = [{classifier_data[model_key]["ci"][0]:.2f}, {classifier_data[model_key]["ci"][1]:.2f}])')

plt.plot([0, 1], [0, 1], color='navy', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

###### SHAP XBG TOP 50
import shap
import numpy as np
import pandas as pd

shap_dataset_classifier_data = {}

# Define the classifiers to analyze
classifiers = ['xgb']
dataset = file_name[0]

# Calculate SHAP values and find top 50 features for each model
for classifier in classifiers:
    cf_data = dataset_classifier_data[dataset][classifier]

    # Initialize SHAP explainer and compute SHAP values
    explainer = shap.TreeExplainer(cf_data['classifier']) if classifier != 'lasso' else shap.LinearExplainer(cf_data['classifier'], X_test)
    shap_values = explainer.shap_values(X_test)

    # Handle different shapes of SHAP values based on classifier type
    if classifier == 'rf':
        # For Random Forest, average the SHAP values across all trees and select the positive class
        shap_values_class = np.mean([shap_value[:, 1] for shap_value in shap_values], axis=0)
    elif isinstance(shap_values, list) and len(shap_values) == 2:
        # For binary classifiers (like XGB and LGB), select the SHAP values for the positive class
        shap_values_class = shap_values[1]
    else:
        # For other classifiers (like LASSO), use the computed SHAP values directly
        shap_values_class = shap_values

    # Convert SHAP values to DataFrame and store it
    shap_values_df = pd.DataFrame(shap_values_class, columns=X_test.columns)
    shap_dataset_classifier_data.setdefault(dataset, {})
    shap_dataset_classifier_data[dataset].setdefault(classifier, {})
    shap_dataset_classifier_data[dataset][classifier]['shap_values'] = shap_values_class
    shap_dataset_classifier_data[dataset][classifier]['shap_values_df'] = shap_values_df

    # Finding the top 50 features
    abs_mean_values = np.abs(shap_values_df).mean()
    sorted_features = abs_mean_values.sort_values(ascending=False)
    top_50_features = sorted_features[:50].index.values
    total_contribution = sorted_features[:50].values.sum()
    count = len(top_50_features)
    bottom_features = abs_mean_values.nsmallest(count).index.tolist()
    bottom_features_contribution = abs_mean_values[bottom_features].sum() / abs_mean_values.sum()
    top_features_contribution = total_contribution / abs_mean_values.sum()
    tb_ratio = top_features_contribution / bottom_features_contribution

    # Print the selected features, their cumulative contribution, and odds ratio
    print(f"Classifier: {classifier}")
    print("Top 50 Selected Features:", top_50_features)
    print("Number of Features:", len(top_50_features))
    print("% of Total Features:", count / len(abs_mean_values) * 100)
    print("Cumulative Contribution of Top Features:", top_features_contribution)
    print("Cumulative Contribution of Bottom Features:", bottom_features_contribution)
    print("Ratio of Top to Bottom Features:", tb_ratio)
    print()

    shap_dataset_classifier_data[dataset][classifier]['top_40_features'] = top_50_features

# Store results in a DataFrame for easy inspection
results = []

for classifier in classifiers:
    top_features = shap_dataset_classifier_data[dataset][classifier]['top_50_features']
    results.append({'Classifier': classifier, 'Top Features': top_features})

results_df = pd.DataFrame(results)
print(results_df)

##SHAP LGB TOP 50 FEATURES
import shap
import pandas as pd
import numpy as np
import lightgbm as lgb

# Assuming dataset_classifier_data and X_test are defined elsewhere
# X_test is your test dataset

# Select the best LightGBM model from the dataset_classifier_data
dataset = file_name[0]
cf_data = dataset_classifier_data[dataset]['lgb']
best_classifier = 'lgb'

# Load SHAP explainer for the LightGBM model
explainer = shap.TreeExplainer(cf_data['classifier'])
if isinstance(cf_data['classifier'], lgb.LGBMClassifier):
    explainer.model.original_model.params['objective'] = 'binary'

shap_values = explainer.shap_values(X_test)

# Handle binary classification
if isinstance(shap_values, list) and len(shap_values) > 1:
    shap_values = shap_values[1]  # use the shap values for the positive class

# Create a DataFrame for the SHAP values
shap_values_df = pd.DataFrame(shap_values, columns=X_test.columns, index=X_test.index)

# Calculate the mean of absolute values for each column
abs_mean_values = np.abs(shap_values_df).mean()

# Sort the features by absolute mean values in descending order
sorted_features = abs_mean_values.sort_values(ascending=False)

# Select the top 50 features
top_50_features = sorted_features[:50].index.values
total_contribution = sorted_features[:50].values.sum()

# Calculate the top-bottom ratio
count = len(top_50_features)
bottom_features = abs_mean_values.nsmallest(count).index.tolist()
bottom_features_contribution = abs_mean_values[bottom_features].sum() / abs_mean_values.sum()
top_features_contribution = total_contribution / abs_mean_values.sum()
tb_ratio = top_features_contribution / bottom_features_contribution

# Print the selected features, their cumulative contribution, and odds ratio
print("Top 50 Selected Features:", top_50_features)
print("Number of Features:", len(top_50_features))
print("% of Total Features:", count / len(abs_mean_values) * 100)
print("Cumulative Contribution of Top Features:", top_features_contribution)
print("Cumulative Contribution of Bottom Features:", bottom_features_contribution)
print("Ratio of Top to Bottom Features:", tb_ratio)
print()

# Store results in a DataFrame for easy inspection
results_df = pd.DataFrame({
    'Top Features': top_50_features,
    'Contribution': sorted_features[:50].values
})

print(results_df)


##SHAP LASSO TOP 50

import shap

# Initialize the explainer with the trained Lasso model
explainer = shap.LinearExplainer(classifier_data['lasso']['classifier'], X_train, feature_perturbation="interventional")
shap_values = explainer.shap_values(X_train)

# Sum the absolute SHAP values for each feature across all samples to get feature importance
shap_abs = np.abs(shap_values).mean(axis=0)
shap_importance = pd.DataFrame({
    'feature': X_train.columns,
    'importance': shap_abs
}).sort_values(by='importance', ascending=False)

# Get the top 50 features
top_50_features = shap_importance.head(50)

# Print the top 50 features
print(top_50_features)

# Visualize the SHAP values for the top 50 features
shap.summary_plot(shap_values[:, top_50_features.index], X_train[top_50_features['feature']])

# Optionally, plot individual feature importance
shap.bar_plot(shap_values[:, top_50_features.index], feature_names=top_50_features['feature'])




